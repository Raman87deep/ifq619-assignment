{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c87e49",
   "metadata": {},
   "source": [
    "# IFQ619 Assignment 1 – Part A\n",
    "**Name:** [Your Name]  \n",
    "**Student ID:** [Your Student ID]  \n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates foundational data analytics techniques using the OSMI 2016 mental health survey dataset (sample).  \n",
    "It addresses **Criterion 1 (Verification)** and **Criterion 2 (Basic techniques)** through loading, verifying, cleaning, analysing, and visualising survey data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188813a4",
   "metadata": {},
   "source": [
    "## Step 1 — Load the dataset\n",
    "Load the CSV dataset into a pandas DataFrame. This allows us to inspect, clean, and analyse the survey responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"osmi_2016.csv\")\n",
    "\n",
    "# Preview shape and first rows\n",
    "df.shape, df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07b252",
   "metadata": {},
   "source": [
    "## Step 2 — Verification and cleaning\n",
    "We verify the dataset by checking data types, missing values, and duplicates. Since all fields are categorical survey responses, we apply light cleaning:\n",
    "- Normalise Yes/No/Maybe style answers\n",
    "- Leave graded responses unchanged\n",
    "- Prepare for analysis and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Missing values and duplicates\n",
    "missing = df.isna().sum()\n",
    "duplicates = df.duplicated().sum()\n",
    "\n",
    "print(\"Missing values per column:\\n\", missing)\n",
    "print(\"\\nDuplicate rows:\", duplicates)\n",
    "\n",
    "# Simple normalisation for Yes/No/Maybe\n",
    "def norm_yn(x):\n",
    "    if not isinstance(x, str):\n",
    "        return np.nan\n",
    "    s = x.strip().lower()\n",
    "    if s in [\"yes\", \"y\", \"true\", \"t\"]:\n",
    "        return \"Yes\"\n",
    "    if s in [\"no\", \"n\", \"false\", \"f\"]:\n",
    "        return \"No\"\n",
    "    if s in [\"maybe\", \"not sure\", \"unsure\", \"don't know\", \"do not know\", \"dk\", \"na\"]:\n",
    "        return \"Unsure / Maybe\"\n",
    "    return x.strip().title()\n",
    "\n",
    "clean = df.copy()\n",
    "for col in clean.columns:\n",
    "    clean[col] = clean[col].apply(norm_yn)\n",
    "\n",
    "clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87caa19b",
   "metadata": {},
   "source": [
    "## Step 3 — Descriptive statistics\n",
    "We calculate frequency tables (counts and percentages) for each question. This provides an overview of response patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Frequency tables for all columns\n",
    "for col in clean.columns:\n",
    "    vc = clean[col].value_counts(dropna=False).to_frame(name=\"count\")\n",
    "    vc[\"percent\"] = (vc[\"count\"] / len(clean) * 100).round(1)\n",
    "    print(f\"\\nFrequency table – {col}:\\n\", vc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2120eae",
   "metadata": {},
   "source": [
    "## Step 4 — Visualisations\n",
    "We use simple bar charts (matplotlib) to visualise response distributions.\n",
    "- One bar chart per question\n",
    "- Cross-tabs for selected relationships (row-normalised)\n",
    "\n",
    "Plots are unstyled as required (default matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot bar chart for each column\n",
    "for col in clean.columns:\n",
    "    plt.figure()\n",
    "    clean[col].value_counts(dropna=False).plot(kind=\"bar\")\n",
    "    plt.title(f\"Distribution – {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example cross-tabs: willingness vs perceived consequences\n",
    "\n",
    "pairs = [\n",
    "    (\"willing_to_discuss_with_supervisor\", \"negative_consequence_supervisor\"),\n",
    "    (\"willing_to_discuss_with_coworkers\", \"negative_consequence_coworkers\"),\n",
    "    (\"benefits_mental_health_coverage\", \"aware_of_mental_health_resources\")\n",
    "]\n",
    "\n",
    "for a, b in pairs:\n",
    "    if a in clean.columns and b in clean.columns:\n",
    "        ct = pd.crosstab(clean[a], clean[b], normalize=\"index\").round(2)\n",
    "        print(f\"\\nCross-tab (row-normalised) – {a} vs {b}:\\n\", ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f84dae",
   "metadata": {},
   "source": [
    "## Step 5 — Notes for verification\n",
    "- I can demonstrate dataset loading, inspection, cleaning, and analysis steps live in the verification session.\n",
    "- I normalised Yes/No/Maybe style responses for consistency.\n",
    "- I created frequency tables and visualisations to address Criterion 1 and 2.\n",
    "- A cleaned CSV has been saved for reproducibility.\n",
    "\n",
    "---\n",
    "**End of Part A**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
